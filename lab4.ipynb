{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read details of the environment & model from a text file\n",
    "Read: \n",
    "\n",
    "- Width, Height\n",
    "- Noise: probability of not going in the intended direction (then divided by two for two unintentional neighbor directions)\n",
    "- Immediate rewards of non-goal states\n",
    "- Terminal (goal) states: their locations and rewards\n",
    "- Internal Walls: locations\n",
    "\n",
    "Create the transition matrix (T) and the Rewards matrix according to above details.\n",
    "\n",
    "\"grid1.txt\" corresponds to the example in AIMA and the lecture slides. You can double-check various results there.\n",
    "\n",
    "Python style comments in grid1.txt are just comments, can be ignored/deleted.\n",
    "\n",
    "Task1 - We ask you to create two more grids (files) of moderate size and with at least one internal wall. And perform the analysis described below on these three grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 360, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filenames = [\"grid1.txt\", \"grid2.txt\", \"grid3.txt\"]\n",
    "W_all = []\n",
    "H_all = []\n",
    "T_all = []\n",
    "Rewards_all = []\n",
    "Terminals_all = []\n",
    "Walls_all = []\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "\n",
    "    file1 = open(filename, 'r')\n",
    "    (W, H) = [t(s) for t,s in zip((int,int),file1.readline().split())]\n",
    "    noise = [t(s) for t,s in zip((float,str),file1.readline().split())][0]\n",
    "    immediate_rewards = [t(s) for t,s in zip((float,str),file1.readline().split())][0]\n",
    "    N = W*H #total number of states\n",
    "    NA = 4 #number of actions\n",
    "    T = np.zeros((N,N,NA)) #state transition probabilities\n",
    "    Directions = np.array(['L','R','U','D'])\n",
    "    Rewards = np.zeros((N,1)) + immediate_rewards #immediate rewards\n",
    "    Terminals = np.zeros((N,1)) #terminal states\n",
    "    Walls = np.zeros((N,1)) #(internal) wall states\n",
    "    prob1 = 1-noise\n",
    "    prob2 = noise/2\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.find('T') != -1:\n",
    "            (_,x, y, r) = [t(s) for t,s in zip((str,int,int,float),line.split())]\n",
    "            Terminals[y*W+x] = 1\n",
    "            Rewards[y*W+x] = r\n",
    "        if line.find('W') != -1:\n",
    "            (_,x, y) = [t(s) for t,s in zip((str,int,int),line.split())]\n",
    "            Walls[y*W+x] = 1\n",
    "    file1.close()\n",
    "    for i in range(W):\n",
    "        for j in range(H):\n",
    "            if i+1<W and Walls[j*W+i+1]==0: #not wall on the right\n",
    "                T[j*W+i,j*W+i+1,1] += prob1\n",
    "                T[j*W+i,j*W+i+1,2] += prob2\n",
    "                T[j*W+i,j*W+i+1,3] += prob2\n",
    "            else:\n",
    "                T[j*W+i,j*W+i,1] += prob1\n",
    "                T[j*W+i,j*W+i,2] += prob2\n",
    "                T[j*W+i,j*W+i,3] += prob2\n",
    "            if i-1>=0 and Walls[j*W+i-1]==0: #not wall on the left\n",
    "                T[j*W+i,j*W+i-1,0] += prob1\n",
    "                T[j*W+i,j*W+i-1,2] += prob2\n",
    "                T[j*W+i,j*W+i-1,3] += prob2\n",
    "            else:\n",
    "                T[j*W+i,j*W+i,0] += prob1\n",
    "                T[j*W+i,j*W+i,2] += prob2\n",
    "                T[j*W+i,j*W+i,3] += prob2\n",
    "            if j+1<H and Walls[(j+1)*W+i]==0: #not wall below\n",
    "                T[j*W+i,(j+1)*W+i,3] += prob1\n",
    "                T[j*W+i,(j+1)*W+i,0] += prob2\n",
    "                T[j*W+i,(j+1)*W+i,1] += prob2\n",
    "            else:\n",
    "                T[j*W+i,j*W+i,3] += prob1\n",
    "                T[j*W+i,j*W+i,0] += prob2\n",
    "                T[j*W+i,j*W+i,1] += prob2\n",
    "            if j-1>=0 and Walls[(j-1)*W+i]==0: #not wall above\n",
    "                T[j*W+i,(j-1)*W+i,2] += prob1\n",
    "                T[j*W+i,(j-1)*W+i,0] += prob2\n",
    "                T[j*W+i,(j-1)*W+i,1] += prob2\n",
    "            else:\n",
    "                T[j*W+i,j*W+i,2] += prob1\n",
    "                T[j*W+i,j*W+i,0] += prob2\n",
    "                T[j*W+i,j*W+i,1] += prob2\n",
    "\n",
    "    W_all.append(W)\n",
    "    H_all.append(H)\n",
    "    T_all.append(T)\n",
    "    Rewards_all.append(Rewards)\n",
    "    Terminals_all.append(Terminals)\n",
    "    Walls_all.append(Walls)\n",
    "\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Implement functions\n",
    "\n",
    "### Value and Policy iteration\n",
    "Value iteration is provided, implement policy iteration\n",
    "\n",
    "### TD-Learning and Q-Learning\n",
    "Q-Learning is implemented, implement TD-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma, T, Rewards, Terminals, Walls):   \n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N,1))\n",
    "    Policy = np.zeros((N,1), dtype=int)\n",
    "    while True:\n",
    "        V_old = np.copy(V)\n",
    "        for s in range(N):\n",
    "            if Walls[s]==1: continue \n",
    "            if Terminals[s]==1:\n",
    "                V[s] = Rewards[s]\n",
    "                continue\n",
    "            Q = np.zeros((NA,1))\n",
    "            # Bellman equation\n",
    "            for a in range(NA):\n",
    "                Q[a] = Rewards[s] + gamma*np.dot(T[s,:,a],V)\n",
    "            V[s] = np.max(Q)\n",
    "            Policy[s] = np.argmax(Q)\n",
    "        if np.sum(np.abs(V-V_old))<1e-10:\n",
    "            break\n",
    "    return V, Policy\n",
    "\n",
    "def policy_iteration(gamma, T, Rewards, Terminals, Walls): \n",
    "    # Initialization\n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N,1))\n",
    "    Policy = np.zeros((N,1), dtype=int)\n",
    "    policy_stable = False\n",
    "\n",
    "    while not policy_stable: # policy iteration\n",
    "\n",
    "        # Policy Evaluation\n",
    "        while True: # value iteration\n",
    "            V_old = np.copy(V)\n",
    "            for s in range(N):\n",
    "                if Walls[s]==1: continue\n",
    "                if Terminals[s]==1:\n",
    "                    V[s] = Rewards[s]\n",
    "                    continue\n",
    "\n",
    "                V[s] = Rewards[s] + gamma*np.dot(T[s,:,Policy[s]],V) # Utility approximation\n",
    "\n",
    "            delta = np.max(np.abs(V-V_old))\n",
    "            if delta < 0.05: # smaller delta won't happen: wrong calculations?\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        Q = np.zeros((NA,1))\n",
    "        for s in range(N):\n",
    "            b = np.copy(Policy[s])\n",
    "            for a in range(NA):\n",
    "                Q[a] = Rewards[s] + gamma*np.dot(T[s,:,a],V) #\n",
    "                # Q[a] = gamma*np.dot(T[s,:,a],V) # expected utility for action a in state s, according to T and V\n",
    "            Policy[s] = np.argmax(Q)\n",
    "            if b != Policy[s]:\n",
    "                policy_stable = False\n",
    "\n",
    "    return V, Policy\n",
    "\n",
    "\n",
    "def TD_learning(N_episodes, Policy, alpha, gamma, T, Rewards, Terminals, Walls):\n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N,1))\n",
    "\n",
    "    for e in range(N_episodes):\n",
    "        # sampling process\n",
    "        s = int(np.floor(np.random.uniform(0,N-1))) # random starting position\n",
    "        while Terminals[s]==1 or Walls[s]==1:\n",
    "            s = int(np.floor(np.random.uniform(0,N-1))) # new random starting position\n",
    "        while Terminals[s]==0:\n",
    "            a = Policy[s] # choose action according to policy which is evaluated\n",
    "\n",
    "            # This logic was provided in Q_earning()\n",
    "            # seems to incorporate the real transition probabilities to get s'\n",
    "            u = np.random.uniform(0,1)\n",
    "            s1 = np.argmax(u<np.cumsum(T[s,:,a]))\n",
    "\n",
    "            V[s] += alpha*(Rewards[s1] + gamma*V[s1] - V[s]) # Policy evaluation (State Utility Estimation)\n",
    "            s = s1\n",
    "\n",
    "    V[Terminals==1] = Rewards[Terminals==1]\n",
    "\n",
    "    return V, Policy\n",
    "\n",
    "def Q_learning(N_episodes, epsilon, alpha, gamma, T, Rewards, Terminals, Walls): \n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    Q = np.zeros((N,NA))\n",
    "    for e in range(N_episodes):\n",
    "        s = int(np.floor(np.random.uniform(0,N-1)))\n",
    "        while Terminals[s]==1 or Walls[s]==1:\n",
    "            s = int(np.floor(np.random.uniform(0,N-1)))\n",
    "        while Terminals[s]==0:\n",
    "            u = np.random.uniform(0,1)\n",
    "            if u<epsilon:\n",
    "                a = int(np.floor(np.random.uniform(0,NA)))\n",
    "            else:\n",
    "                a = np.argmax(Q[s,:])\n",
    "            u = np.random.uniform(0,1)\n",
    "            s1 = np.argmax(u<np.cumsum(T[s,:,a]))\n",
    "            Q[s,a] += alpha*(Rewards[s1] + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "            s = s1\n",
    "        epsilon = epsilon*0.9999 # Modify here for other annealing regimes\n",
    "        alpha = alpha*0.9999 # Modify here for other annealing regimes\n",
    "    V = np.max(Q,axis=1)\n",
    "    V = V[:,None]\n",
    "    V[Terminals==1] = Rewards[Terminals==1] \n",
    "    Policy = np.array(np.argmax(Q,axis=1)).reshape((N,1))\n",
    "    return V, Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Experiments\n",
    "\n",
    "For the three grids you have, perform the following analysis and include them in your report,.\n",
    "\n",
    "- Compare value and policy iterations in terms of convergence time. Relate it to computational complexity, current implementation and number of iterations needed.\n",
    "\n",
    "- How are optimal policies change with immediate reward values? Show some examples (similar to Figure 17.2b in AIMA)\n",
    "\n",
    "- Compare TD-Learning and Q-Learning results with each other. Also with value and policy iterations. Remember that value and policy iteration solve Markov Decision Processes where we know the model (T and Rewards). TD-Learning and Q-Learning are (passive and active, respectively) Reinforcement Learning methods that don't have the model but use data from simulations. Data are simulated from the model we know, but the model is not used n TD or Q-Learning.\n",
    "\n",
    "- What are the effects of epsilon and alpha values and how they are modified?\n",
    "\n",
    "- What is the effect of number of episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "gamma = 1\n",
    "alpha = 0.9\n",
    "epsilon = 0.5\n",
    "N_episodes = 20000\n",
    "\n",
    "V_VI_all, Policy_VI_all, Time_VI_all = [], [], []\n",
    "V_PI_all, Policy_PI_all, Time_PI_all = [], [], []\n",
    "V_TD_all, Policy_TD_all, Time_TD_all = [], [], []\n",
    "V_Q_all, Policy_Q_all, Time_Q_all = [], [], []\n",
    "\n",
    "for grid in range(len(W_all)):\n",
    "    # select which grid to use\n",
    "    W = W_all[grid]\n",
    "    H = H_all[grid]\n",
    "    T = T_all[grid]\n",
    "    Rewards = Rewards_all[grid]\n",
    "    Terminals = Terminals_all[grid]\n",
    "    Walls = Walls_all[grid]\n",
    "\n",
    "\n",
    "    #Value iteration\n",
    "    st = time.time()\n",
    "    V_VI, Policy_VI = value_iteration(gamma, T, Rewards, Terminals, Walls)\n",
    "    et = time.time()\n",
    "    V_VI_all.append(V_VI)\n",
    "    Policy_VI_all.append(Policy_VI)\n",
    "    Time_VI_all.append(et-st)\n",
    "\n",
    "    #Policy iteration\n",
    "    st = time.time()\n",
    "    V_PI, Policy_PI = policy_iteration(gamma, T, Rewards, Terminals, Walls)\n",
    "    et = time.time()\n",
    "    V_PI_all.append(V_PI)\n",
    "    Policy_PI_all.append(Policy_PI)\n",
    "    Time_PI_all.append(et-st)\n",
    "\n",
    "    #TD-Learning\n",
    "    st = time.time()\n",
    "    V_TD, Policy_TD = TD_learning(N_episodes, Policy_VI, alpha, gamma, T, Rewards, Terminals, Walls)\n",
    "    et = time.time()\n",
    "    V_TD_all.append(V_TD)\n",
    "    Policy_TD_all.append(Policy_TD)\n",
    "    Time_TD_all.append(et-st)\n",
    "\n",
    "    #Q-Learning\n",
    "    st = time.time()\n",
    "    V_Q, Policy_Q = Q_learning(N_episodes, epsilon, alpha, gamma, T, Rewards, Terminals, Walls)\n",
    "    et = time.time()\n",
    "    V_Q_all.append(V_Q)\n",
    "    Policy_Q_all.append(Policy_Q)\n",
    "    Time_Q_all.append(et-st)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"RESULTS Grid:\", grid, \" ###################################################################\")\n",
    "    print(\"RESULTS: VI\")\n",
    "    print(V_VI.reshape((H,W)))\n",
    "    maze = Directions[Policy_VI.astype(int)]\n",
    "    maze[Walls==1] = 'W'\n",
    "    maze[Terminals==1] = 'G'\n",
    "    print(maze.reshape((H,W)))\n",
    "    #\n",
    "    print(\"RESULTS: PI\")\n",
    "    print(V_PI.reshape((H,W)))\n",
    "    maze = Directions[Policy_PI.astype(int)]\n",
    "    maze[Walls==1] = 'W'\n",
    "    maze[Terminals==1] = 'G'\n",
    "    print(maze.reshape((H,W)))\n",
    "    #\n",
    "    print(\"RESULTS: Q-L\")\n",
    "    print(V_Q.reshape((H,W)))\n",
    "    maze_Q = Directions[Policy_Q.astype(int)]\n",
    "    maze_Q[Walls==1] = 'W'\n",
    "    maze_Q[Terminals==1] = 'G'\n",
    "    print(maze_Q.reshape((H,W)))\n",
    "    #\n",
    "    print(\"RESULTS: TD-L\")\n",
    "    print(V_TD.reshape((H,W)))\n",
    "    maze = Directions[Policy_TD.astype(int)]\n",
    "    maze[Walls==1] = 'W'\n",
    "    maze[Terminals==1] = 'G'\n",
    "    print(maze.reshape((H,W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid: 0  ###################################################################\n",
      "\n",
      "[[' ' ' ' ' ' 'G']\n",
      " [' ' 'W' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ']]\n",
      "Grid: 1  ###################################################################\n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'W' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'W' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'W' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'W' ' ' ' ' ' ' 'G' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' 'G' ' ']]\n",
      "Grid: 2  ###################################################################\n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'W' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'W' 'W' 'W' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' 'G' ' ' 'W']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "# Illustrate grids without policy\n",
    "\n",
    "for grid in range(len(W_all)):\n",
    "    # select which grid to use\n",
    "    W = W_all[grid]\n",
    "    H = H_all[grid]\n",
    "    T = T_all[grid]\n",
    "    Rewards = Rewards_all[grid]\n",
    "    Terminals = Terminals_all[grid]\n",
    "    Walls = Walls_all[grid]\n",
    "\n",
    "\n",
    "    print(\"Grid:\", grid, \" ###################################################################\")\n",
    "    print(\"\")\n",
    "    maze = np.full_like(Walls, fill_value=' ', dtype=str)\n",
    "    maze[Walls==1] = 'W'\n",
    "    maze[Terminals==1] = 'G'\n",
    "    print(maze.reshape((H,W)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (extra credit) - What if diagonal moves were possible?\n",
    "\n",
    "Repeat the experiments in one of these settings. You don't have to stick to the code provided here. But, you can only use Numpy (and matplotlib).\n",
    "\n",
    "You should decide on how to distribute the noise associated with actions across (next) states. For example, in the scenario above we have probability of going Up with the Up action is 1-noise. The probability of going Left with the Up action is noise/2. The same for going Right."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}